---
title: "Code_SVM_qui fonctionne"
author: "Mireile"
date: "2024-10-19"
output: html_document
---

```{r setup, include=FALSE}
# les librairys
library(e1071)
library(caret)
library(ggplot2)
library(reshape2)
library(kernlab)
```
# Lecture des données
```{r }
data <- read.csv("Maternal Health Risk Data Set.csv")
```

# Afficher les premières lignes et structure des données

```{r }
head(data)
str(data)
```

Factorisation de RiskLevel
```{r}
data$RiskLevel <- as.factor(data$RiskLevel)
str(data)
```




Split de la data en 2
```{r}
set.seed(123)
index <- createDataPartition(data$RiskLevel, p = 0.7, list = FALSE)
train_data <- data[index, ]
test_data <- data[-index, ]
```

Classsification multiclass: 
svm de e1071
Syntaxe : svm_model_One_vs_One <- svm(RiskLevel ~ ., data = train_data, method = "C-classification", kernel = "linear")
Package : e1071
Method : "C-classification" spécifie que c'est un problème de classification.
Kernel : Le noyau spécifié est "linear", ce qui signifie que le modèle va chercher une frontière linéaire entre les classes.
Stratégie multi-classes : Par défaut, pour un problème multi-classes, le SVM dans e1071 utilise une stratégie One-vs-One (OvO), où plusieurs classifieurs binaires sont entraînés pour chaque paire de classes.

```{r}


```



Test avec le 1er modèle SVM One-vs-one
```{r}
library(e1071)
# Entraînement du modèle SVM avec l'approche One-vs-One
svm_model_One_vs_One <- svm(RiskLevel ~ ., data = train_data, method = "C-classification", kernel = "linear")

predictions_One_vs_One <- predict(svm_model_One_vs_One, newdata = test_data)

# Calcul de la matrice de confusion
conf_matrix_One_vs_One <- confusionMatrix(predictions_One_vs_One, test_data$RiskLevel)
print(conf_matrix_One_vs_One)
cm_table_One_vs_One <- as.table(conf_matrix_One_vs_One)

# Conversion de la matrice de confusion en data frame pour ggplot
cm_df_One_vs_One <- as.data.frame(cm_table_One_vs_One)
colnames(cm_df_One_vs_One) <- c("Predicted", "Actual", "Freq")

# Plot de la heatmap de la matrice de confusion
ggplot(data = cm_df_One_vs_One, aes(x = Actual, y = Predicted, fill = Freq)) +
  geom_tile(color = "white") +
  geom_text(aes(label = Freq), vjust = 1) +
  scale_fill_gradient(low = "white", high = "blue") +
  labs(title = "Matrice de Confusion_One_vs_One_linéaire", x = "Classe Réelle", y = "Classe Prédite_One_vs_One_linéaire") +
  theme_minimal()
```

2ième test toujours avec le modele OVO mais cette fois-ci radial
```{r}
library(e1071)
# Entraînement du modèle SVM avec l'approche One-vs-One et 
# Activer la prédiction probabiliste lors de l'entraînement du modèle SVM
svm_model_One_vs_One_radial <- svm(RiskLevel ~ ., data = train_data, method = "C-classification", kernel = "radial", 
                            probability = TRUE)

predictions_One_vs_One <- predict(svm_model_One_vs_One_radial, newdata = test_data, probability = TRUE)

# Calcul de la matrice de confusion
conf_matrix_One_vs_One <- confusionMatrix(predictions_One_vs_One, test_data$RiskLevel)
print(conf_matrix_One_vs_One)
cm_table_One_vs_One <- as.table(conf_matrix_One_vs_One)

# Conversion de la matrice de confusion en data frame pour ggplot
cm_df_One_vs_One <- as.data.frame(cm_table_One_vs_One)
colnames(cm_df_One_vs_One) <- c("Predicted", "Actual", "Freq")

# Plot de la heatmap de la matrice de confusion
ggplot(data = cm_df_One_vs_One, aes(x = Actual, y = Predicted, fill = Freq)) +
  geom_tile(color = "white") +
  geom_text(aes(label = Freq), vjust = 1) +
  scale_fill_gradient(low = "white", high = "violet") +
  labs(title = "Matrice de Confusion_One_vs_One_radial", x = "Classe Réelle", y = "Classe Prédite_One_vs_One_radial") +
  theme_minimal()
```
Interpretation:
En observant les 2 modeles linaire vs radial on voit une légère amélioration de l'accurancy avec le model radiale


###########################################################################################################
Classsification multiclass: 

ksvm de kernlab
Syntaxe : svm_model <- ksvm(RiskLevel ~ ., data = train_data, type = "C-svc", kernel = "vanilladot")
Package : kernlab
Type : "C-svc" est un service de classification, équivalent à "C-classification" dans e1071.
Kernel : Le noyau "vanilladot" est l'équivalent du noyau linéaire.
Stratégie multi-classes : Par défaut, ksvm utilise une stratégie One-vs-Rest (OvR) pour les problèmes multi-classes, où un classifieur binaire est entraîné pour chaque classe, contre toutes les autres.
Différences clés :
Stratégie multi-classes :

(e1071::svm : Utilise une stratégie One-vs-One par défaut.
kernlab::ksvm : Utilise une stratégie One-vs-Rest par défaut.)




3ieme Test avec le 2ime modèle SVM One-vs-all
test avec le kernel linéaire
```{r}
# Dans cette étape, nous allons entraîner le modèle avec un kernel linéaire.
# Entraîner le modèle SVM One-vs-All
svm_model <- ksvm(RiskLevel ~ ., data = train_data, type = "C-svc", kernel = "vanilladot")

# Prédire les labels de l'ensemble de test
predictions <- predict(svm_model, newdata = test_data)

# Évaluer la performance avec une matrice de confusion
conf_matrix <- confusionMatrix(predictions, test_data$RiskLevel)
print(conf_matrix)

# matrice de confusion
# Charger les bibliothèques nécessaires
library(ggplot2)
library(caret)

# Extraire les données de la matrice de confusion sous forme de tableau
conf_data <- as.data.frame(conf_matrix$table)
colnames(conf_data) <- c("Prédiction", "Vrai", "Freq")

# Visualiser la matrice de confusion sous forme de heatmap
ggplot(conf_data, aes(x = Prédiction, y = Vrai, fill = Freq)) +
  geom_tile(color = "white") +
  scale_fill_gradient(low = "pink", high = "red") +
  geom_text(aes(label = Freq), color = "white", size = 5) +
  labs(title = "Matrice de Confusion", y = "Classe Prédite_OVA_linéaire", x = "Classe Réelle") +
  theme_minimal()


```

2ième test toujours avec le modele OVA mais cette fois-ci radial

```{r}
svm_model_OVA_radial <- ksvm(RiskLevel ~ ., 
                             data = train_data, 
                             type = "C-svc", 
                             kernel = "rbfdot", 
                             prob.model = TRUE)

# Prédire les labels de l'ensemble de test
predictions <- predict(svm_model_OVA_radial, newdata = test_data)

# Évaluer la performance avec une matrice de confusion
conf_matrix <- confusionMatrix(predictions, test_data$RiskLevel)
print(conf_matrix)

# matrice de confusion
# Charger les bibliothèques nécessaires
library(ggplot2)
library(caret)

# Extraire les données de la matrice de confusion sous forme de tableau
conf_data <- as.data.frame(conf_matrix$table)
colnames(conf_data) <- c("Prédiction", "Vrai", "Freq")

# Visualiser la matrice de confusion sous forme de heatmap
ggplot(conf_data, aes(x = Prédiction, y = Vrai, fill = Freq)) +
  geom_tile(color = "white") +
  scale_fill_gradient(low = "yellow", high = "goldenrod") +
  geom_text(aes(label = Freq), color = "white", size = 5) +
  labs(title = "Matrice de Confusion_OVA_Radial", y = "Classe Prédite_OVA_Radial", x = "Classe Réelle") +
  theme_minimal()

```
# pareil que pour le modele ovo on a une meilleur accurancy avec le kernel radial, 
et comprarer au modele svm ovo, le modele ova est légèrement mieux

############################################################################
Afin de comparer la performance de nos différentes modèles nous allons utilser la courbe ROC

encodage avant tracer de la courbe et j'ai aussi centré et réduit

```{r}
levels(train_data$RiskLevel)
```
```{r}
levels(test_data$RiskLevel)
```

```{r}

```



```{r}
# Recode de la variable de sortie (One-vs-Rest) pour train_data

# 1. Pour la classe "low risk" vs les autres ("mid risk" + "high risk")
train_data$low_vs_rest <- ifelse(train_data$RiskLevel == "low risk", "low risk", "rest")

# 2. Pour la classe "mid risk" vs les autres ("low risk" + "high risk")
train_data$mid_vs_rest <- ifelse(train_data$RiskLevel == "mid risk", "mid risk", "rest")

# 3. Pour la classe "high risk" vs les autres ("low risk" + "mid risk")
train_data$high_vs_rest <- ifelse(train_data$RiskLevel == "high risk", "high risk", "rest")

# Vérification du nouveau format de train_data
head(train_data)

```

```{r}
str(train_data$low_vs_rest)

```

```{r}
train_data$low_vs_rest <- as.factor(train_data$low_vs_rest)
str(train_data)
```
```{r}
train_data$mid_vs_rest <- as.factor(train_data$mid_vs_rest)
str(train_data)
```
```{r}
train_data$high_vs_rest <- as.factor(train_data$high_vs_rest)
str(train_data)
```

```{r}
# Sélectionner les colonnes prédictives (en excluant la variable cible)
predictors <- train_data[, !names(train_data) %in% c("RiskLevel", "low_vs_rest", "mid_vs_rest", "high_vs_rest")]

# Mettre à l'échelle les variables prédictives
scaled_predictors_train <- scale(predictors)

# Créer un nouveau DataFrame avec les variables mises à l'échelle et la variable cible
train_data_scaled <- data.frame(low_vs_rest = train_data$low_vs_rest, scaled_predictors_train)

# Vérifier les dimensions du nouveau DataFrame
head(train_data_scaled)
str(train_data_scaled)

```
# 1er modalité regardé "low risk"

```{r}
# Vérifier les valeurs manquantes dans train_data_scaled
sum(is.na(train_data_scaled))

```

```{r}
# Vérifier le type de la variable
str(train_data_scaled$low_vs_rest)

```
```{r}
# Entraîner le modèle SVM avec ksvm
svm_model_low <- ksvm(low_vs_rest ~ ., 
                      data = train_data_scaled, 
                      type = "C-svc", 
                      kernel = "rbfdot", 
                      prob.model = TRUE)
```




```{r}
# Recode de la variable de sortie (One-vs-Rest) pour test_data

# 1. Pour la classe "low risk" vs les autres ("mid risk" + "high risk")
test_data$low_vs_rest <- ifelse(test_data$RiskLevel == "low risk", "low risk", "rest")

# 2. Pour la classe "mid risk" vs les autres ("low risk" + "high risk")
test_data$mid_vs_rest <- ifelse(test_data$RiskLevel == "mid risk", "mid risk", "rest")

# 3. Pour la classe "high risk" vs les autres ("low risk" + "mid risk")
test_data$high_vs_rest <- ifelse(test_data$RiskLevel == "high risk", "high risk", "rest")

# Vérification du nouveau format de train_data
head(test_data)
```

```{r}
str(test_data$low_vs_rest)
```

```{r}
test_data$low_vs_rest <- as.factor(test_data$low_vs_rest)
str(test_data)
```

```{r}
test_data$mid_vs_rest <- as.factor(test_data$mid_vs_rest)
str(test_data)
```

```{r}
test_data$high_vs_rest <- as.factor(test_data$high_vs_rest)
str(test_data)
```


```{r}
# Sélectionner les colonnes prédictives (en excluant la variable cible)
predictors <- test_data[, !names(train_data) %in% c("RiskLevel", "low_vs_rest", "mid_vs_rest", "high_vs_rest")]

# Mettre à l'échelle les variables prédictives
scaled_predictors_test <- scale(predictors)

# Créer un nouveau DataFrame avec les variables mises à l'échelle et la variable cible
test_data_scaled_test_low <- data.frame(low_vs_rest = test_data$low_vs_rest, scaled_predictors_test)

# Vérifier les dimensions du nouveau DataFrame
head(test_data_scaled_test_low)
str(test_data_scaled_test_low)
```
```{r}
# chargement de la librairie pROC
library(pROC)

```

```{r}
# Effectuer les prédictions et stocker les probabilités dans la variable 'prob'
prob <- predict(svm_model_low, newdata = test_data_scaled_test_low, type = "probabilities")

```


```{r}
# Les probabilités classe (low risk)
prob_low <- prob[, "low risk"]

```

```{r}
# Création des courbes ROC pour chaque classe
# roc_low <- roc(test_data_scaled$low_vs_rest, prob_low)
roc_low <- roc(test_data_scaled_test_low$low_vs_rest, prob_low, levels = c("low risk","rest" ), direction = ">")
```


```{r}
# Tracer la première courbe ROC (low risk vs les autres)
plot(roc_low, col = "blue", main = "Courbes ROC classe low risk", lwd = 2)
```
```{r}
# Évaluer la performance avec une matrice de confusion
# str(test_data_scaled_test_low$low_vs_rest)

predicted_classes <- ifelse(prob_low > 0.5, "low risk", "rest")
predicted_classes <- factor(predicted_classes, levels = c("low risk", "rest"))
true_classes <- factor(test_data_scaled_test_low$low_vs_rest, levels = c("low risk", "rest"))
conf_matrix <- confusionMatrix(predicted_classes, true_classes)


# matrice de confusion
# Charger les bibliothèques nécessaires
library(ggplot2)
library(caret)

# Extraire les données de la matrice de confusion sous forme de tableau
conf_data <- as.data.frame(conf_matrix$table)
colnames(conf_data) <- c("Prédiction", "Vrai", "Freq")


print(conf_matrix)
# Visualiser la matrice de confusion sous forme de heatmap
ggplot(conf_data, aes(x = Prédiction, y = Vrai, fill = Freq)) +
  geom_tile(color = "white") +
  scale_fill_gradient(low = "lightblue", high = "blue") +
  geom_text(aes(label = Freq), color = "white", size = 5) +
  labs(title = "Matrice de Confusion_OVA_lowRisk", y = "Classe Prédite_OVA_Radial", x = "Classe Réelle") +
  theme_minimal()
```


# deuxième variable en one versus all
```{r}

# Créer un nouveau DataFrame avec les variables mises à l'échelle et la variable cible
train_data_scaled_mid <- data.frame(mid_vs_rest = train_data$mid_vs_rest, scaled_predictors_train)

# Vérifier les dimensions du nouveau DataFrame
head(train_data_scaled_mid)
str(train_data_scaled_mid)
```

```{r}

# Créer un nouveau DataFrame avec les variables mises à l'échelle et la variable cible
test_data_scaled_mid <- data.frame(mid_vs_rest = test_data$mid_vs_rest, scaled_predictors_test)

# Vérifier les dimensions du nouveau DataFrame
head(test_data_scaled_mid)

```

```{r}
# Entraîner le modèle SVM avec ksvm
svm_model_mid <- ksvm(mid_vs_rest ~ ., 
                      data = train_data_scaled_mid, 
                      type = "C-svc", 
                      kernel = "rbfdot", 
                      prob.model = TRUE)
```



```{r}

prob_mid <- predict(svm_model_mid, newdata = test_data_scaled_mid, type = "probabilities")
# Les probabilités de la classe mid risk
prob_mid <- prob_mid[, "mid risk"]
# Création des courbes ROC pour chaque classe
# roc_low <- roc(test_data_scaled$low_vs_rest, prob_low)
roc_mid_vs_rest <- roc(test_data_scaled_mid$mid_vs_rest, prob_mid, levels = c("mid risk","rest" ), direction = ">")

# Tracer la première courbe ROC (low risk vs les autres)
plot(roc_low, col = "orange", main = "Courbes ROC classe mid risk", lwd = 2)
```
```{r}
# Évaluer la performance avec une matrice de confusion
# str(test_data_scaled_test_low$low_vs_rest)

predicted_classes <- ifelse(prob_mid > 0.5, "mid risk", "rest")
predicted_classes <- factor(predicted_classes, levels = c("mid risk", "rest"))
true_classes <- factor(test_data_scaled_mid$mid_vs_rest, levels = c("mid risk", "rest"))
conf_matrix <- confusionMatrix(predicted_classes, true_classes)


# matrice de confusion
# Charger les bibliothèques nécessaires
library(ggplot2)
library(caret)

# Extraire les données de la matrice de confusion sous forme de tableau
conf_data <- as.data.frame(conf_matrix$table)
colnames(conf_data) <- c("Prédiction", "Vrai", "Freq")


print(conf_matrix)
# Visualiser la matrice de confusion sous forme de heatmap
ggplot(conf_data, aes(x = Prédiction, y = Vrai, fill = Freq)) +
  geom_tile(color = "white") +
  scale_fill_gradient(low = "yellow", high = "goldenrod") +
  geom_text(aes(label = Freq), color = "white", size = 5) +
  labs(title = "Matrice de Confusion_OVA_midRisk", y = "Classe Prédite_OVA_Radial", x = "Classe Réelle") +
  theme_minimal()
```



```{r}

# Créer un nouveau DataFrame avec les variables mises à l'échelle et la variable cible
train_data_scaled_high <- data.frame(high_vs_rest = train_data$high_vs_rest, scaled_predictors_train)

# Vérifier les dimensions du nouveau DataFrame
head(train_data_scaled_high)
str(train_data_scaled_high)


# Créer un nouveau DataFrame avec les variables mises à l'échelle et la variable cible
test_data_scaled_high <- data.frame(high_vs_rest = test_data$high_vs_rest, scaled_predictors_test)

# Vérifier les dimensions du nouveau DataFrame
head(test_data_scaled_high)


# Entraîner le modèle SVM avec ksvm
svm_model_high <- ksvm(high_vs_rest ~ ., 
                      data = train_data_scaled_high, 
                      type = "C-svc", 
                      kernel = "rbfdot", 
                      prob.model = TRUE)
```


```{r}

prob_high <- predict(svm_model_high, newdata = test_data_scaled_high, type = "probabilities")
# Les probabilités pour la classe high risk
prob_high <- prob_high[, "high risk"]
# Création des courbes ROC pour chaque classe
# roc_low <- roc(test_data_scaled$low_vs_rest, prob_low)
roc_high <- roc(test_data_scaled_high$high_vs_rest, prob_high, levels = c("high risk","rest" ), direction = ">")

# Tracer la première courbe ROC (low risk vs les autres)
plot(roc_high, col = "green", main = "Courbes ROC pour différentes classes", lwd = 2)
```


```{r}
# Évaluer la performance avec une matrice de confusion
# str(test_data_scaled_test_low$low_vs_rest)

predicted_classes <- ifelse(prob_high > 0.5, "high risk", "rest")
predicted_classes <- factor(predicted_classes, levels = c("high risk", "rest"))
true_classes <- factor(test_data_scaled_high$high_vs_rest, levels = c("high risk", "rest"))
conf_matrix <- confusionMatrix(predicted_classes, true_classes)


# matrice de confusion
# Charger les bibliothèques nécessaires
library(ggplot2)
library(caret)

# Extraire les données de la matrice de confusion sous forme de tableau
conf_data <- as.data.frame(conf_matrix$table)
colnames(conf_data) <- c("Prédiction", "Vrai", "Freq")


print(conf_matrix)
# Visualiser la matrice de confusion sous forme de heatmap
ggplot(conf_data, aes(x = Prédiction, y = Vrai, fill = Freq)) +
  geom_tile(color = "white") +
  scale_fill_gradient(low = "lightgreen", high = "green") +
  geom_text(aes(label = Freq), color = "white", size = 5) +
  labs(title = "Matrice de Confusion_OVA_highRisk", y = "Classe Prédite_OVA_Radial", x = "Classe Réelle") +
  theme_minimal()

```




```{r}
# Sélectionner les colonnes prédictives (en excluant la variable cible)
predictors <- train_data[, !names(train_data) %in% c("RiskLevel", "low_vs_rest", "mid_vs_rest", "high_vs_rest")]

# Mettre à l'échelle les variables prédictives
scaled_predictors_train <- scale(predictors)

# Créer un nouveau DataFrame avec les variables mises à l'échelle et la variable cible
train_data_scaled2 <- data.frame(RiskLevel = train_data$RiskLevel, scaled_predictors_train)

# Vérifier les dimensions du nouveau DataFrame
head(train_data_scaled2)
str(train_data_scaled2)
```
```{r}
# Créer un nouveau DataFrame avec les variables mises à l'échelle et la variable cible
test_data_scaled2 <- data.frame(RiskLevel = test_data$RiskLevel, scaled_predictors_test)

# Vérifier les dimensions du nouveau DataFrame
head(test_data_scaled2)
```

# test avec le modele one vs one sur data scale
```{r}
library(e1071)
# Entraînement du modèle SVM avec l'approche One-vs-One et 
# Activer la prédiction probabiliste lors de l'entraînement du modèle SVM
svm_model_One_vs_One_radial3 <- svm(RiskLevel ~ ., data = train_data_scaled2, method = "C-classification", kernel = "radial", 
                            probability = TRUE)

predictions_One_vs_One3 <- predict(svm_model_One_vs_One_radial3, newdata = test_data_scaled2, probability = TRUE)

# Calcul de la matrice de confusion
conf_matrix_One_vs_One3 <- confusionMatrix(predictions_One_vs_One3, test_data_scaled2$RiskLevel)
print(conf_matrix_One_vs_One3)
cm_table_One_vs_One3 <- as.table(conf_matrix_One_vs_One3)

# Conversion de la matrice de confusion en data frame pour ggplot
cm_df_One_vs_One3 <- as.data.frame(cm_table_One_vs_One3)
colnames(cm_df_One_vs_One3) <- c("Predicted", "Actual", "Freq")

# Plot de la heatmap de la matrice de confusion
ggplot(data = cm_df_One_vs_One3, aes(x = Actual, y = Predicted, fill = Freq)) +
  geom_tile(color = "white") +
  geom_text(aes(label = Freq), vjust = 1) +
  scale_fill_gradient(low = "white", high = "violet") +
  labs(title = "Matrice de Confusion_One_vs_One_radial", x = "Classe Réelle", y = "Classe Prédite_One_vs_One_radial") +
  theme_minimal()
```

```{r}
str(test_data_scaled2$RiskLevel)
```



```{r}
# Prédire les probabilités sur l'ensemble de test
prob_predictions4 <- predict(svm_model_One_vs_One_radial3, newdata = test_data_scaled2, probability = TRUE)
# Convertir les probabilités en une matrice (si ce n'est pas déjà fait)
prob_matrix <- attr(prob_predictions4, "probabilities")
# Extraire la classe avec la probabilité maximale
predicted_classes <- apply(prob_matrix, 1, function(x) names(x)[which.max(x)])
# Afficher les classes prédites
print(predicted_classes)

# Créer les courbes ROC pour chaque classe
# 1. Pour la classe 'high risk'
roc_high <- roc(response = as.numeric(test_data_scaled2$RiskLevel == "high risk"), 
                predictor = prob_matrix[, "high risk"])


# 2. Pour la classe 'low risk'
roc_low <- roc(response = as.numeric(test_data_scaled2$RiskLevel == "low risk"), 
               predictor = prob_matrix[, "low risk"])

# 3. Pour la classe 'mid risk'
roc_mid <- roc(response = as.numeric(test_data_scaled2$RiskLevel == "mid risk"), 
               predictor = prob_matrix[, "mid risk"])

# Tracer les courbes ROC
plot(roc_high, col = "red", main = "Courbes ROC2 pour chaque classe", lwd = 2)
plot(roc_low, col = "blue", add = TRUE, lwd = 2)
plot(roc_mid, col = "green", add = TRUE, lwd = 2)

# # Ajouter une légende pour identifier chaque courbe
# legend("bottomright", legend = c("High Risk", "Low Risk", "Mid Risk"), col = c("red", "blue", "green"), lwd = 2)
# Ajouter une légende pour identifier chaque courbe
legend("bottomright", 
       legend = c(paste("High Risk (AUC =", round(auc_high, 2), ")"), 
                  paste("Low Risk (AUC =", round(auc_low, 2), ")"), 
                  paste("Mid Risk (AUC =", round(auc_mid, 2), ")")), 
       col = c("red", "blue", "green"), lwd = 2)


# Afficher l'AUC pour chaque classe
auc_high <- auc(roc_high)
auc_low <- auc(roc_low)
auc_mid <- auc(roc_mid)
cat("AUC High Risk:", auc_high, "\nAUC Low Risk:", auc_low, "\nAUC Mid Risk:", auc_mid, "\n")
```


```{r}
summary(prob_matrix[, "high risk"])  # Vérifiez la distribution des probabilités

```


```{r}
unique(test_data_scaled2$RiskLevel)  # Vérifiez les valeurs uniques

```





